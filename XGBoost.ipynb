{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/subarnasaikia/podcasting-xgboost?scriptVersionId=232124588\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"from IPython.display import display\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:12.816324Z","iopub.execute_input":"2025-04-05T19:47:12.816773Z","iopub.status.idle":"2025-04-05T19:47:14.071458Z","shell.execute_reply.started":"2025-04-05T19:47:12.816731Z","shell.execute_reply":"2025-04-05T19:47:14.070365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reading csv files","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/playground-series-s5e4/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s5e4/test.csv\")\nsample_submission_df = pd.read_csv(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:14.073585Z","iopub.execute_input":"2025-04-05T19:47:14.074014Z","iopub.status.idle":"2025-04-05T19:47:16.092064Z","shell.execute_reply.started":"2025-04-05T19:47:14.073986Z","shell.execute_reply":"2025-04-05T19:47:16.090841Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Details of the data tables","metadata":{}},{"cell_type":"code","source":"def df_details(df):\n    print(\"------------------------------------------------\")\n    print(\"df shape :\", df.shape)\n    print(\"------------------------------------------------\")\n\n    print(\"\\n------------------------------------------------\")\n    print(\"df top 5 data :\")\n    print(\"------------------------------------------------\")\n    display(df.head(5))\n\n\n    print(\"\\n\\n------------------------------------------------\")\n    print(\"df info :\")\n    print(\"------------------------------------------------\")\n    display(df.info())\n\n\n    print(\"\\n\\n------------------------------------------------\")\n    print(\"df describe numeric data :\")\n    print(\"------------------------------------------------\")\n    display(df.describe())\n\n    obs_cols = df.select_dtypes(include='object').columns\n    if len(obs_cols) > 0:\n        print(\"\\n\\n------------------------------------------------\")\n        print(\"df describe object data :\")\n        print(\"------------------------------------------------\")\n        display(df.describe(include=object))\n    else:\n        print(\"\\n\\n------------------------------------------------\")\n        print(\"No object data available :\")\n        print(\"------------------------------------------------\")\n\n    \n    print(\"\\n\\n------------------------------------------------\")\n    print(\"Missing Values :\")\n    print(\"------------------------------------------------\")\n    print( df.isnull().sum()[df.isnull().sum() > 0] )\n    \n    \n    missing_percentage = (df.isnull().sum() / len(df)) * 100 \n    print(\"\\n\\n------------------------------------------------\")\n    print(\"Percentage of Missing values: (%) \")\n    print(\"------------------------------------------------\")\n    print(missing_percentage[missing_percentage > 0])\n    \n\n    \n    total_missing_percentage = (df.isnull().sum().sum() / (df.size)) * 100\n    print(\"\\n\\n------------------------------------------------\")\n    print(f\"Total missing values percentage: {total_missing_percentage:.2f}%\")\n    print(\"------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:16.093637Z","iopub.execute_input":"2025-04-05T19:47:16.093936Z","iopub.status.idle":"2025-04-05T19:47:16.103418Z","shell.execute_reply.started":"2025-04-05T19:47:16.093913Z","shell.execute_reply":"2025-04-05T19:47:16.102124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n****************************************************\")\nprint(\"Details of train_df: \")\nprint(\"\\n****************************************************\")\ndf_details(train_df)\n\nprint(\"\\n\\n\\n\\n****************************************************\")\nprint(\"Details of test_df: \")\nprint(\"\\n****************************************************\")\ndf_details(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:16.104607Z","iopub.execute_input":"2025-04-05T19:47:16.104923Z","iopub.status.idle":"2025-04-05T19:47:18.930459Z","shell.execute_reply.started":"2025-04-05T19:47:16.104894Z","shell.execute_reply":"2025-04-05T19:47:18.929114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling missing values and objects fields","metadata":{}},{"cell_type":"markdown","source":"### One Hot encoding","metadata":{}},{"cell_type":"code","source":"def encoding(X_train, X_valid, test):\n    cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n    print(\"\\n------------------------------------------------\")\n    print(f\"Categorical columns: {cat_cols}\")\n    print(\"------------------------------------------------\")\n\n    # Fill missing values\n    for df in [X_train, X_valid, test]:\n        df[cat_cols] = df[cat_cols].fillna('missing')\n\n    # Split columns by number of unique values in X_train\n    onehot_cols = [col for col in cat_cols if X_train[col].nunique() <= 10]\n    ordinal_cols = [col for col in cat_cols if X_train[col].nunique() > 10]\n\n    # Encoded datasets, dropiong the categorical columns as we will encode it\n    X_train_encoded = X_train.drop(cat_cols, axis=1).reset_index(drop=True)\n    X_valid_encoded = X_valid.drop(cat_cols, axis=1).reset_index(drop=True)\n    test_encoded = test.drop(cat_cols, axis=1).reset_index(drop=True)\n\n    # One-Hot Encoding for categorical cols that have less than 11 unique values\n    if onehot_cols:\n        onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n        onehot_encoder.fit(X_train[onehot_cols])\n\n        cols = onehot_encoder.get_feature_names_out(onehot_cols)\n\n        X_train_1hot = pd.DataFrame(onehot_encoder.transform(X_train[onehot_cols]), columns=cols)\n        X_valid_1hot = pd.DataFrame(onehot_encoder.transform(X_valid[onehot_cols]), columns=cols)\n        test_1hot = pd.DataFrame(onehot_encoder.transform(test[onehot_cols]), columns=cols)\n\n        X_train_encoded = pd.concat([X_train_encoded, X_train_1hot], axis=1)\n        X_valid_encoded = pd.concat([X_valid_encoded, X_valid_1hot], axis=1)\n        test_encoded = pd.concat([test_encoded, test_1hot], axis=1)\n\n    # Ordinal Encoding for categorical cols that have more than 10 unique values\n    if ordinal_cols:\n        ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n        ordinal_encoder.fit(X_train[ordinal_cols])\n\n        X_train_ord = pd.DataFrame(ordinal_encoder.transform(X_train[ordinal_cols]), columns=ordinal_cols)\n        X_valid_ord = pd.DataFrame(ordinal_encoder.transform(X_valid[ordinal_cols]), columns=ordinal_cols)\n        test_ord = pd.DataFrame(ordinal_encoder.transform(test[ordinal_cols]), columns=ordinal_cols)\n\n        X_train_encoded = pd.concat([X_train_encoded, X_train_ord], axis=1)\n        X_valid_encoded = pd.concat([X_valid_encoded, X_valid_ord], axis=1)\n        test_encoded = pd.concat([test_encoded, test_ord], axis=1)\n\n    return X_train_encoded, X_valid_encoded, test_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:18.93167Z","iopub.execute_input":"2025-04-05T19:47:18.932078Z","iopub.status.idle":"2025-04-05T19:47:18.945255Z","shell.execute_reply.started":"2025-04-05T19:47:18.93204Z","shell.execute_reply":"2025-04-05T19:47:18.943603Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputation","metadata":{}},{"cell_type":"code","source":"def imputation(X_train, X_valid, test, strategy='mean'):\n    my_imputer = SimpleImputer(strategy=strategy)\n    \n    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n    imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n    imputed_test = pd.DataFrame(my_imputer.transform(test))\n    \n    imputed_X_train.columns = X_train.columns\n    imputed_X_valid.columns = X_valid.columns\n    imputed_test.columns = test.columns\n    \n    return imputed_X_train, imputed_X_valid, imputed_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:18.946485Z","iopub.execute_input":"2025-04-05T19:47:18.946781Z","iopub.status.idle":"2025-04-05T19:47:18.969641Z","shell.execute_reply.started":"2025-04-05T19:47:18.946757Z","shell.execute_reply":"2025-04-05T19:47:18.968466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Scaling","metadata":{}},{"cell_type":"code","source":"def scaling(X_train, X_valid, test):\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_valid_scaled = scaler.transform(X_valid)\n    test_scaled = scaler.transform(test)\n\n    X_train_data = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n    X_valid_data = pd.DataFrame(X_valid_scaled, columns=X_valid.columns)\n    test_data = pd.DataFrame(test_scaled, columns=test.columns)\n    \n    return X_train_data, X_valid_data, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:18.970766Z","iopub.execute_input":"2025-04-05T19:47:18.971081Z","iopub.status.idle":"2025-04-05T19:47:18.991323Z","shell.execute_reply.started":"2025-04-05T19:47:18.971055Z","shell.execute_reply":"2025-04-05T19:47:18.989768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Splitting dataset","metadata":{}},{"cell_type":"code","source":"y = train_df.Listening_Time_minutes\nX = train_df.drop(['Listening_Time_minutes', 'id'], axis=1)\ntest_df = test_df.drop(['id'], axis=1)\n\nprint(\"X shape : \", X.shape)\nprint(\"y shape : \",y.shape)\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nprint(\"X_train shape : \",X_train.shape)\nprint(\"y_train shape : \",y_train.shape)\nprint(\"X_valid shape : \",X_valid.shape)\nprint(\"y_valid shape : \",y_valid.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:18.994381Z","iopub.execute_input":"2025-04-05T19:47:18.994732Z","iopub.status.idle":"2025-04-05T19:47:19.280541Z","shell.execute_reply.started":"2025-04-05T19:47:18.994705Z","shell.execute_reply":"2025-04-05T19:47:19.279146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Categorical Encoding...\")\nX_train , X_valid, test_df = encoding(X_train, X_valid, test_df)\nprint(\"Numerical imputation...\")\nX_train , X_valid, test_df = imputation(X_train, X_valid, test_df)\nprint(\"Feature Scaling...\")\nX_train , X_valid, test_df = scaling(X_train, X_valid, test_df)\n\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_valid shape: \", X_valid.shape)\nprint(\"test_df shape: \", test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:19.282204Z","iopub.execute_input":"2025-04-05T19:47:19.282753Z","iopub.status.idle":"2025-04-05T19:47:24.297542Z","shell.execute_reply.started":"2025-04-05T19:47:19.282713Z","shell.execute_reply":"2025-04-05T19:47:24.296356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n****************************************************\")\nprint(\"Details of X_train: \")\nprint(\"\\n****************************************************\")\ndf_details(X_train)\n\nprint(\"\\n****************************************************\")\nprint(\"Details of X_valid: \")\nprint(\"\\n****************************************************\")\ndf_details(X_valid)\n\nprint(\"\\n****************************************************\")\nprint(\"Details of test_df: \")\nprint(\"\\n****************************************************\")\ndf_details(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:47:24.298726Z","iopub.execute_input":"2025-04-05T19:47:24.299052Z","iopub.status.idle":"2025-04-05T19:47:26.406361Z","shell.execute_reply.started":"2025-04-05T19:47:24.299025Z","shell.execute_reply":"2025-04-05T19:47:26.405077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGBoost","metadata":{}},{"cell_type":"code","source":"model = XGBRegressor(\n    n_estimators=100000, \n    learning_rate=0.054321,\n    max_depth=6,               # Adjust tree depth\n    min_child_weight=1,        # Tune to control overfitting\n    subsample=0.8,             # Fraction of samples used per tree\n    colsample_bytree=0.8,      # Fraction of features used per tree\n    reg_alpha=0.1,             # L1 regularization\n    reg_lambda=1.0,            # L2 regularization\n    objective='reg:squarederror',  # Suitable objective for regression\n    random_state=42  \n)\n\nmodel.fit(X_train, y_train, \n        eval_set=[(X_valid, y_valid)],\n          eval_metric='rmse',\n          callbacks=[EarlyStopping(rounds=10)],\n         verbose=True\n    )","metadata":{"trusted":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-04-05T19:47:26.407629Z","iopub.execute_input":"2025-04-05T19:47:26.407951Z","iopub.status.idle":"2025-04-05T19:49:11.117035Z","shell.execute_reply.started":"2025-04-05T19:47:26.407923Z","shell.execute_reply":"2025-04-05T19:49:11.116135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict(X_valid)\nmae = mean_absolute_error(y_valid, y_pred)\nmse = mean_squared_error(y_valid, y_pred)\nrmse = np.sqrt(mse)\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"MAE: {mae:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T19:51:13.280169Z","iopub.execute_input":"2025-04-05T19:51:13.280613Z","iopub.status.idle":"2025-04-05T19:51:13.448044Z","shell.execute_reply.started":"2025-04-05T19:51:13.280583Z","shell.execute_reply":"2025-04-05T19:51:13.446336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_prediction = model.predict(test_df).flatten()\nsubmission = pd.DataFrame({'id': sample_submission_df['id'], 'Listening_Time_minutes': final_prediction})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-05T19:46:58.651Z"}},"outputs":[],"execution_count":null}]}