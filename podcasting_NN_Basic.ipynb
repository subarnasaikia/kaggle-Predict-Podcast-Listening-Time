{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/subarnasaikia/podcasting?scriptVersionId=232064183\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"from IPython.display import display\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:34.441097Z","iopub.execute_input":"2025-04-05T13:18:34.44147Z","iopub.status.idle":"2025-04-05T13:18:48.844757Z","shell.execute_reply.started":"2025-04-05T13:18:34.441434Z","shell.execute_reply":"2025-04-05T13:18:48.843704Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reading csv files","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/playground-series-s5e4/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s5e4/test.csv\")\nsample_submission_df = pd.read_csv(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:48.845805Z","iopub.execute_input":"2025-04-05T13:18:48.846493Z","iopub.status.idle":"2025-04-05T13:18:51.616657Z","shell.execute_reply.started":"2025-04-05T13:18:48.846453Z","shell.execute_reply":"2025-04-05T13:18:51.615878Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Details of the data tables","metadata":{}},{"cell_type":"code","source":"def df_details(df):\n    print(\"------------------------------------------------\")\n    print(\"df shape :\", df.shape)\n    print(\"------------------------------------------------\")\n\n    print(\"\\n------------------------------------------------\")\n    print(\"df top 5 data :\")\n    print(\"------------------------------------------------\")\n    display(df.head(5))\n\n\n    print(\"\\n\\n------------------------------------------------\")\n    print(\"df info :\")\n    print(\"------------------------------------------------\")\n    display(df.info())\n\n\n    print(\"\\n\\n------------------------------------------------\")\n    print(\"df describe numeric data :\")\n    print(\"------------------------------------------------\")\n    display(df.describe())\n\n    obs_cols = df.select_dtypes(include='object').columns\n    if len(obs_cols) > 0:\n        print(\"\\n\\n------------------------------------------------\")\n        print(\"df describe object data :\")\n        print(\"------------------------------------------------\")\n        display(df.describe(include=object))\n    else:\n        print(\"\\n\\n------------------------------------------------\")\n        print(\"No object data available :\")\n        print(\"------------------------------------------------\")\n\n    \n    print(\"\\n\\n------------------------------------------------\")\n    print(\"Missing Values :\")\n    print(\"------------------------------------------------\")\n    print( df.isnull().sum()[df.isnull().sum() > 0] )\n    \n    \n    missing_percentage = (df.isnull().sum() / len(df)) * 100 \n    print(\"\\n\\n------------------------------------------------\")\n    print(\"Percentage of Missing values: (%) \")\n    print(\"------------------------------------------------\")\n    print(missing_percentage[missing_percentage > 0])\n    \n\n    \n    total_missing_percentage = (df.isnull().sum().sum() / (df.size)) * 100\n    print(\"\\n\\n------------------------------------------------\")\n    print(f\"Total missing values percentage: {total_missing_percentage:.2f}%\")\n    print(\"------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:51.61756Z","iopub.execute_input":"2025-04-05T13:18:51.617798Z","iopub.status.idle":"2025-04-05T13:18:51.62567Z","shell.execute_reply.started":"2025-04-05T13:18:51.617778Z","shell.execute_reply":"2025-04-05T13:18:51.624704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n****************************************************\")\nprint(\"Details of train_df: \")\nprint(\"\\n****************************************************\")\ndf_details(train_df)\n\nprint(\"\\n\\n\\n\\n****************************************************\")\nprint(\"Details of test_df: \")\nprint(\"\\n****************************************************\")\ndf_details(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:51.627498Z","iopub.execute_input":"2025-04-05T13:18:51.627727Z","iopub.status.idle":"2025-04-05T13:18:54.382798Z","shell.execute_reply.started":"2025-04-05T13:18:51.627707Z","shell.execute_reply":"2025-04-05T13:18:54.381732Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling missing values and objects fields","metadata":{}},{"cell_type":"markdown","source":"### One Hot encoding","metadata":{}},{"cell_type":"code","source":"def encoding(X_train, X_valid, test):\n    cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n    print(\"\\n------------------------------------------------\")\n    print(f\"Categorical columns: {cat_cols}\")\n    print(\"------------------------------------------------\")\n\n    # Fill missing values\n    for df in [X_train, X_valid, test]:\n        df[cat_cols] = df[cat_cols].fillna('missing')\n\n    # Split columns by number of unique values in X_train\n    onehot_cols = [col for col in cat_cols if X_train[col].nunique() <= 10]\n    ordinal_cols = [col for col in cat_cols if X_train[col].nunique() > 10]\n\n    # Encoded datasets, dropiong the categorical columns as we will encode it\n    X_train_encoded = X_train.drop(cat_cols, axis=1).reset_index(drop=True)\n    X_valid_encoded = X_valid.drop(cat_cols, axis=1).reset_index(drop=True)\n    test_encoded = test.drop(cat_cols, axis=1).reset_index(drop=True)\n\n    # One-Hot Encoding for categorical cols that have less than 11 unique values\n    if onehot_cols:\n        onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n        onehot_encoder.fit(X_train[onehot_cols])\n\n        cols = onehot_encoder.get_feature_names_out(onehot_cols)\n\n        X_train_1hot = pd.DataFrame(onehot_encoder.transform(X_train[onehot_cols]), columns=cols)\n        X_valid_1hot = pd.DataFrame(onehot_encoder.transform(X_valid[onehot_cols]), columns=cols)\n        test_1hot = pd.DataFrame(onehot_encoder.transform(test[onehot_cols]), columns=cols)\n\n        X_train_encoded = pd.concat([X_train_encoded, X_train_1hot], axis=1)\n        X_valid_encoded = pd.concat([X_valid_encoded, X_valid_1hot], axis=1)\n        test_encoded = pd.concat([test_encoded, test_1hot], axis=1)\n\n    # Ordinal Encoding for categorical cols that have more than 10 unique values\n    if ordinal_cols:\n        ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n        ordinal_encoder.fit(X_train[ordinal_cols])\n\n        X_train_ord = pd.DataFrame(ordinal_encoder.transform(X_train[ordinal_cols]), columns=ordinal_cols)\n        X_valid_ord = pd.DataFrame(ordinal_encoder.transform(X_valid[ordinal_cols]), columns=ordinal_cols)\n        test_ord = pd.DataFrame(ordinal_encoder.transform(test[ordinal_cols]), columns=ordinal_cols)\n\n        X_train_encoded = pd.concat([X_train_encoded, X_train_ord], axis=1)\n        X_valid_encoded = pd.concat([X_valid_encoded, X_valid_ord], axis=1)\n        test_encoded = pd.concat([test_encoded, test_ord], axis=1)\n\n    return X_train_encoded, X_valid_encoded, test_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:54.38434Z","iopub.execute_input":"2025-04-05T13:18:54.384597Z","iopub.status.idle":"2025-04-05T13:18:54.393783Z","shell.execute_reply.started":"2025-04-05T13:18:54.384576Z","shell.execute_reply":"2025-04-05T13:18:54.392978Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputation","metadata":{}},{"cell_type":"code","source":"def imputation(X_train, X_valid, test, strategy='mean'):\n    my_imputer = SimpleImputer(strategy=strategy)\n    \n    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n    imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n    imputed_test = pd.DataFrame(my_imputer.transform(test))\n    \n    imputed_X_train.columns = X_train.columns\n    imputed_X_valid.columns = X_valid.columns\n    imputed_test.columns = test.columns\n    \n    return imputed_X_train, imputed_X_valid, imputed_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:54.39464Z","iopub.execute_input":"2025-04-05T13:18:54.394865Z","iopub.status.idle":"2025-04-05T13:18:54.411645Z","shell.execute_reply.started":"2025-04-05T13:18:54.394845Z","shell.execute_reply":"2025-04-05T13:18:54.410846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Scaling","metadata":{}},{"cell_type":"code","source":"def scaling(X_train, X_valid, test):\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_valid_scaled = scaler.transform(X_valid)\n    test_scaled = scaler.transform(test)\n\n    X_train_data = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n    X_valid_data = pd.DataFrame(X_valid_scaled, columns=X_valid.columns)\n    test_data = pd.DataFrame(test_scaled, columns=test.columns)\n    \n    return X_train_data, X_valid_data, test_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:54.412446Z","iopub.execute_input":"2025-04-05T13:18:54.412672Z","iopub.status.idle":"2025-04-05T13:18:54.431206Z","shell.execute_reply.started":"2025-04-05T13:18:54.412653Z","shell.execute_reply":"2025-04-05T13:18:54.430338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Spliting dataset","metadata":{}},{"cell_type":"code","source":"y = train_df.Listening_Time_minutes\nX = train_df.drop(['Listening_Time_minutes', 'id'], axis=1)\ntest_df = test_df.drop(['id'], axis=1)\n\nprint(\"X shape : \", X.shape)\nprint(\"y shape : \",y.shape)\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nprint(\"X_train shape : \",X_train.shape)\nprint(\"y_train shape : \",y_train.shape)\nprint(\"X_valid shape : \",X_valid.shape)\nprint(\"y_valid shape : \",y_valid.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:54.43209Z","iopub.execute_input":"2025-04-05T13:18:54.4324Z","iopub.status.idle":"2025-04-05T13:18:54.672098Z","shell.execute_reply.started":"2025-04-05T13:18:54.432369Z","shell.execute_reply":"2025-04-05T13:18:54.671267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Categorical Encoding...\")\nX_train , X_valid, test_df = encoding(X_train, X_valid, test_df)\nprint(\"Numerical imputation...\")\nX_train , X_valid, test_df = imputation(X_train, X_valid, test_df)\nprint(\"Feature Scaling...\")\nX_train , X_valid, test_df = scaling(X_train, X_valid, test_df)\n\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_valid shape: \", X_valid.shape)\nprint(\"test_df shape: \", test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:54.673063Z","iopub.execute_input":"2025-04-05T13:18:54.673436Z","iopub.status.idle":"2025-04-05T13:18:59.404293Z","shell.execute_reply.started":"2025-04-05T13:18:54.673402Z","shell.execute_reply":"2025-04-05T13:18:59.40336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n****************************************************\")\nprint(\"Details of X_train: \")\nprint(\"\\n****************************************************\")\ndf_details(X_train)\n\nprint(\"\\n****************************************************\")\nprint(\"Details of X_valid: \")\nprint(\"\\n****************************************************\")\ndf_details(X_valid)\n\nprint(\"\\n****************************************************\")\nprint(\"Details of test_df: \")\nprint(\"\\n****************************************************\")\ndf_details(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:18:59.4051Z","iopub.execute_input":"2025-04-05T13:18:59.405427Z","iopub.status.idle":"2025-04-05T13:19:01.285571Z","shell.execute_reply.started":"2025-04-05T13:18:59.405402Z","shell.execute_reply":"2025-04-05T13:19:01.284594Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## NN model","metadata":{}},{"cell_type":"code","source":"def build_nn_model(input_shape):\n    model = keras.Sequential([\n        layers.BatchNormalization(),\n        layers.Dense(1024, activation='relu', kernel_initializer='he_normal', input_shape=[input_shape]),\n        layers.Dropout(0.3),\n        layers.BatchNormalization(),\n        layers.Dense(1024, activation='relu', kernel_initializer='he_normal'),\n        layers.Dropout(0.2),\n        layers.BatchNormalization(),\n        layers.Dense(1024, activation='relu', kernel_initializer='he_normal'),\n        layers.Dropout(0.4),\n        layers.BatchNormalization(),\n        layers.Dense(1024, activation='relu', kernel_initializer='he_normal'),\n        layers.Dropout(0.3),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n        layers.Dropout(0.2),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n        layers.Dropout(0.2),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n        layers.BatchNormalization(),\n        layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n        layers.BatchNormalization(),\n        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),\n        layers.Dense(1) \n    ])\n\n    optimizer = Adam(learning_rate=0.0001)\n    model.compile(\n        optimizer=optimizer,\n        loss='mean_squared_error', \n        metrics=['mae']\n    )\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:19:01.286611Z","iopub.execute_input":"2025-04-05T13:19:01.286968Z","iopub.status.idle":"2025-04-05T13:19:01.29426Z","shell.execute_reply.started":"2025-04-05T13:19:01.286933Z","shell.execute_reply":"2025-04-05T13:19:01.293398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True\n)\n\n\nmodel = build_nn_model(X_train.shape[1])\n\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    epochs=1000,\n    batch_size=512,\n    verbose=1,\n    callbacks=[early_stopping],\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T13:19:01.295162Z","iopub.execute_input":"2025-04-05T13:19:01.295484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss, mae = model.evaluate(X_valid, y_valid)\nprint(f\"Validation MAE: {mae:.2f}\")\nprint(f\"Validation Loss: {loss:.2f}\")\n\ny_pred = model.predict(X_valid)\nmae = mean_squared_error(y_valid, y_pred)\nrmse = np.sqrt(mae)\nprint(f\"Validation RMSE: {rmse:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nprint(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['mae', 'val_mae']].plot();","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_prediction = model.predict(test_df).flatten()\nsubmission = pd.DataFrame({'id': sample_submission_df['id'], 'Listening_Time_minutes': final_prediction})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}